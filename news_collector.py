# main.py
import os, base64, markdown, json, time, random, re, logging, feedparser
from datetime import datetime, timedelta
from email.mime.text import MIMEText
from urllib.parse import urlparse, urljoin
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed

# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from jinja2 import Environment, FileSystemLoader
from PIL import Image
from zoneinfo import ZoneInfo
from newspaper import Article, ArticleException

# êµ¬ê¸€ ì¸ì¦ ê´€ë ¨
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai

from config import Config

# --- ë¡œê¹… ì„¤ì • ---
def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def get_kst_today_str():
    return datetime.now(ZoneInfo('Asia/Seoul')).strftime("%Y-%m-%d")

def markdown_to_html(text):
    return markdown.markdown(text) if text else ""

# --- í•µì‹¬ ê¸°ëŠ¥ í´ë˜ìŠ¤ ---
class NewsScraper:
    def __init__(self, config):
        self.config = config
        self.session = self._create_session()

    def _create_session(self):
        session = requests.Session()
        retry = Retry(total=2, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('https://', adapter)
        session.mount('http://', adapter)
        return session

    def get_image_url(self, soup, article_url):
        try:
            meta_image = soup.find("meta", property="og:image") or soup.find("meta", name="twitter:image")
            if meta_image and meta_image.get("content"):
                meta_url = self._resolve_url(article_url, meta_image["content"])
                if self._validate_image(meta_url): return meta_url

            for tag in soup.select('figure > img, picture > img', limit=5):
                img_url = tag.get('src') or tag.get('data-src') or (tag.get('srcset').split(',')[0].strip().split(' ')[0] if tag.get('srcset') else None)
                if img_url:
                    full_url = self._resolve_url(article_url, img_url)
                    if self._validate_image(full_url): return full_url
            
            for img in soup.find_all("img", limit=10):
                img_url = img.get("src") or img.get("data-src")
                if img_url:
                    full_url = self._resolve_url(article_url, img_url)
                    if self._validate_image(full_url): return full_url
            
            return self.config.DEFAULT_IMAGE_URL
        except Exception:
            return self.config.DEFAULT_IMAGE_URL

    def _resolve_url(self, base_url, image_url):
        if image_url.startswith('//'): return 'https:' + image_url
        return urljoin(base_url, image_url)

    def _validate_image(self, image_url):
        try:
            response = self.session.get(image_url, stream=True, timeout=5)
            response.raise_for_status()
            content_type = response.headers.get('Content-Type', '').lower()
            if 'image' not in content_type: return False
            img_data = BytesIO(response.content)
            with Image.open(img_data) as img:
                width, height = img.size
                if width < self.config.MIN_IMAGE_WIDTH or height < self.config.MIN_IMAGE_HEIGHT: return False
                return True
        except Exception:
            return False

class AIService:
    # (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    def __init__(self, config):
        self.config = config
        if not self.config.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        genai.configure(api_key=self.config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(self.config.GEMINI_MODEL)

    def generate_single_summary(self, article_title: str, article_text: str) -> str:
        logging.info(f"    -> AI ìš”ì•½ ìƒì„± ìš”ì²­: {article_title}")
        if not article_text or len(article_text) < 100:
            logging.warning("      -> âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½ì„ ê±´ë„ˆ<binary data, 2 bytes><binary data, 2 bytes>ë‹ˆë‹¤.")
            return "ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        try:
            prompt = f"ë‹¹ì‹ ì€ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ëŠ” ë‰´ìŠ¤ ì—ë””í„°ì…ë‹ˆë‹¤. ì•„ë˜ ì œëª©ê³¼ ë³¸ë¬¸ì„ ê°€ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ë…ìë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ 3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n[ì œëª©]: {article_title}\n[ë³¸ë¬¸]:\n{article_text[:2000]}"
            response = self.model.generate_content(prompt)
            logging.info(f"      -> âœ… AI ìš”ì•½ ìƒì„± ì„±ê³µ.")
            return response.text.strip()
        except Exception:
            logging.error("      -> ğŸš¨ AI ìš”ì•½ API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ", exc_info=True)
            return "AI ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _generate_content_with_retry(self, prompt, is_json=False):
        for attempt in range(3):
            try:
                response = self.model.generate_content(prompt)
                if is_json:
                    cleaned_text = response.text.strip().replace("```json", "").replace("```", "")
                    json.loads(cleaned_text)
                    return cleaned_text
                return response.text
            except Exception as e:
                logging.warning(f"AI ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3): {e}")
                time.sleep(2 ** attempt)
        return None

    def select_top_news(self, news_list):
        logging.info(f"AI ë‰´ìŠ¤ ì„ ë³„ ì‹œì‘... (ëŒ€ìƒ: {len(news_list)}ê°œ)")
        context = "\n\n".join([f"ê¸°ì‚¬ #{i}\nì œëª©: {news['title']}\nìš”ì•½: {news['summary']}" for i, news in enumerate(news_list)])
        prompt = f"ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ë¬¼ë¥˜ ì „ë¬¸ ë‰´ìŠ¤ í¸ì§‘ì¥ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë…ìì—ê²Œ ê°€ì¥ ê°€ì¹˜ ìˆëŠ” ì •ë³´ë§Œì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‘ ê°€ì§€ ì‘ì—…ì„ ìˆœì„œëŒ€ë¡œ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ì‘ì—… 1: ì£¼ì œë³„ ê·¸ë£¹í™” ë° ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • - ë‚´ìš©ì´ ì‚¬ì‹¤ìƒ ë™ì¼í•œ ë‰´ìŠ¤ë“¤ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìœ¼ì„¸ìš”. (ì˜ˆ: ë™ì¼í•œ ì‚¬ê±´, ë°œí‘œ, ì¸ë¬¼ ì¸í„°ë·° ë“±) - ê° ê·¸ë£¹ì—ì„œ ì œëª©ì´ ê°€ì¥ êµ¬ì²´ì ì´ê³  ìš”ì•½ ì •ë³´ê°€ í’ë¶€í•œ ê¸°ì‚¬ë¥¼ **ë‹¨ í•˜ë‚˜ë§Œ** ëŒ€í‘œë¡œ ì„ ì •í•˜ì„¸ìš”. - **í•˜ë‚˜ì˜ ë™ì¼í•œ ì‚¬ê±´ì— ëŒ€í•´ì„œëŠ” ë°˜ë“œì‹œ ë‹¨ í•˜ë‚˜ì˜ ëŒ€í‘œ ê¸°ì‚¬ë§Œ ìµœì¢… í›„ë³´ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.** ì‘ì—… 2: ìµœì¢… Top 10 ì„ ì • - ëŒ€í‘œ ê¸°ì‚¬ë¡œ ì„ ì •ëœ í›„ë³´ë“¤ ì¤‘ì—ì„œ, ì‹œì¥ ë™í–¥, ê¸°ìˆ  í˜ì‹ , ì£¼ìš” ê¸°ì—… ì†Œì‹ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ìµœì¢… 10ê°œë¥¼ ì„ ì •í•´ì£¼ì„¸ìš”. [ë‰´ìŠ¤ ëª©ë¡]\n{context}\n\n[ì¶œë ¥ í˜•ì‹] - ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. - 'selected_indices' í‚¤ì— ë‹¹ì‹ ì´ ìµœì¢… ì„ ì •í•œ ê¸°ì‚¬ 10ê°œì˜ ë²ˆí˜¸(ì¸ë±ìŠ¤)ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë‹´ì•„ì£¼ì„¸ìš”. ì˜ˆ: {{\"selected_indices\": [3, 15, 4, 8, 22, 1, 30, 11, 19, 5]}}"
        response_text = self._generate_content_with_retry(prompt, is_json=True)
        if response_text:
            try:
                selected_indices = json.loads(response_text).get('selected_indices', [])
                top_news = [news_list[i] for i in selected_indices if i < len(news_list)]
                logging.info(f"âœ… AIê°€ {len(top_news)}ê°œ ë‰´ìŠ¤ë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.")
                return top_news
            except (json.JSONDecodeError, KeyError) as e:
                logging.error(f"âŒ AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}. ìƒìœ„ 10ê°œ ë‰´ìŠ¤ë¥¼ ì„ì˜ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")
        return news_list[:10]

    def generate_briefing(self, news_list):
        logging.info("AI ë¸Œë¦¬í•‘ ìƒì„± ì‹œì‘...")
        context = "\n\n".join([f"ì œëª©: {news['title']}\nìš”ì•½: {news.get('ai_summary') or news.get('summary')}" for news in news_list])
        prompt = f"ë‹¹ì‹ ì€ íƒì›”í•œ í†µì°°ë ¥ì„ ê°€ì§„ IT/ê²½ì œ ë‰´ìŠ¤ íë ˆì´í„°ì…ë‹ˆë‹¤. ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, ë…ìë¥¼ ìœ„í•œ ë§¤ìš° ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ 'ë°ì¼ë¦¬ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. **ì¶œë ¥ í˜•ì‹ ê·œì¹™:** 1. 'ì—ë””í„° ë¸Œë¦¬í•‘'ì€ '## ì—ë””í„° ë¸Œë¦¬í•‘' í—¤ë”ë¡œ ì‹œì‘í•˜ë©°, ì˜¤ëŠ˜ ë‰´ìŠ¤ì˜ í•µì‹¬ì„ 2~3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. 2. 'ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„'ì€ '## ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„' í—¤ë”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. 3. ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„ì—ì„œëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ 2~3ê°œë¥¼ '###' í—¤ë”ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤. 4. ê° ì¹´í…Œê³ ë¦¬ ì•ˆì—ì„œëŠ”, ê´€ë ¨ëœ ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³  ê¸€ë¨¸ë¦¬ ê¸°í˜¸(`*`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 5. ë¬¸ì¥ ì•ˆì—ì„œ ê°•ì¡°í•˜ê³  ì‹¶ì€ íŠ¹ì • í‚¤ì›Œë“œëŠ” í°ë”°ì˜´í‘œ(\" \")ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”. [ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ëª©ë¡]\n{context}"
        briefing = self._generate_content_with_retry(prompt)
        if briefing: logging.info("âœ… AI ë¸Œë¦¬í•‘ ìƒì„± ì„±ê³µ!")
        else: logging.warning("âš ï¸ AI ë¸Œë¦¬í•‘ ìƒì„± ì‹¤íŒ¨.")
        return briefing

class NewsService:
    def __init__(self, config, scraper, ai_service):
        self.config = config
        self.scraper = scraper
        self.ai_service = ai_service
        self.sent_links = self._load_sent_links()

    def _load_sent_links(self):
        try:
            with open(self.config.SENT_LINKS_FILE, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f)
        except FileNotFoundError:
            return set()

    def get_fresh_news(self):
        try:
            all_articles = self._fetch_articles_from_rss_feeds()
            logging.info(f"ì´ {len(all_articles)}ê°œì˜ í›„ë³´ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

            # ì´ë¯¸ ë³´ë‚¸ ë§í¬ì™€ ì¤‘ë³µ ë§í¬ë¥¼ 1ì°¨ë¡œ í•„í„°ë§
            unique_articles = []
            seen_links = set()
            for article in all_articles:
                if article['link'] not in self.sent_links and article['link'] not in seen_links:
                    seen_links.add(article['link'])
                    unique_articles.append(article)
            
            logging.info(f"ì¤‘ë³µ ì œì™¸ í›„ {len(unique_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ê° ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´(ë³¸ë¬¸, ì´ë¯¸ì§€)ë¥¼ ê°€ì ¸ì˜´
            processed_articles = []
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_article = {executor.submit(self._process_single_article, article): article for article in unique_articles}
                for future in as_completed(future_to_article):
                    result = future.result()
                    if result:
                        processed_articles.append(result)
            
            logging.info(f"âœ… ì´ {len(processed_articles)}ê°œì˜ ìœ íš¨í•œ ìƒˆ ë‰´ìŠ¤ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            return processed_articles
        except Exception as e:
            logging.error("âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ:", exc_info=True)
            return []
    
    def _fetch_articles_from_rss_feeds(self):
        """RSS í”¼ë“œ ëª©ë¡ì„ ìˆœíšŒí•˜ë©° ëª¨ë“  ê¸°ì‚¬ í•­ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        all_entries = []
        for feed_url in self.config.RSS_FEEDS:
            try:
                logging.info(f"-> RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {feed_url}")
                feed = feedparser.parse(feed_url, agent=random.choice(self.config.USER_AGENTS))
                cutoff_date = datetime.now(ZoneInfo('UTC')) - timedelta(days=2)
                
                for entry in feed.entries:
                    published_time = datetime(*entry.published_parsed[:6], tzinfo=ZoneInfo('UTC')) if hasattr(entry, 'published_parsed') else datetime.now(ZoneInfo('UTC'))
                    if published_time > cutoff_date:
                        all_entries.append({
                            'title': entry.title,
                            'link': entry.link,
                            'summary': BeautifulSoup(entry.summary, 'lxml').get_text(strip=True) if hasattr(entry, 'summary') else ""
                        })
            except Exception:
                logging.warning(f"  -> âš ï¸ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {feed_url}")
        return all_entries

    def _process_single_article(self, article_info):
        """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ë°›ì•„ í‚¤ì›Œë“œ í•„í„°ë§, ë³¸ë¬¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # 1. í‚¤ì›Œë“œ í•„í„°ë§
            search_text = article_info['title'] + " " + article_info['summary']
            if not any(keyword.lower() in search_text.lower() for keyword in self.config.KEYWORDS):
                return None
            
            logging.info(f"  -> í‚¤ì›Œë“œ ì¼ì¹˜, ì²˜ë¦¬ ì‹œì‘: {article_info['title']}")
            
            # 2. newspaper3kë¥¼ ì´ìš©í•´ ë³¸ë¬¸ ë° ìµœì¢… ì •ë³´ ì¶”ì¶œ
            article = Article(article_info['link'])
            article.download()
            article.parse()
            
            # ìœ íš¨í•˜ì§€ ì•Šì€ ê¸°ì‚¬(ë‚´ìš©ì´ ì—†ê±°ë‚˜ ì œëª©ì´ ì—†ëŠ” ê²½ìš°) ì œì™¸
            if not article.text or not article.title: return None

            # 3. ì´ë¯¸ì§€ ì¶”ì¶œ
            # newspaper3kê°€ íŒŒì‹±í•œ HTML(soup)ì„ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í¼ì— ì „ë‹¬
            image_url = self.scraper.get_image_url(article.soup, article.url)
            
            return {
                'title': article.title,
                'link': article.url,
                'summary': article_info['summary'][:150] + "...",
                'image_url': image_url,
                'full_text': article.text
            }
        except Exception:
            # logging.error(f"  -> ğŸš¨ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {article_info.get('title')}", exc_info=True)
            return None

    def update_sent_links_log(self, news_list):
        links = [news['link'] for news in news_list]
        try:
            with open(self.config.SENT_LINKS_FILE, 'a', encoding='utf-8') as f:
                for link in links: f.write(link + '\n')
            logging.info(f"âœ… {len(links)}ê°œ ë§í¬ë¥¼ ë°œì†¡ ê¸°ë¡ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.error("âŒ ë°œì†¡ ê¸°ë¡ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:", exc_info=True)

class EmailService:
    # (ì´ì „ê³¼ ë™ì¼, ë³€ê²½ ì—†ìŒ)
    def __init__(self, config):
        self.config = config
        self.credentials = self._get_credentials()
    def _get_credentials(self):
        creds = None
        if os.path.exists(self.config.TOKEN_FILE):
            creds = Credentials.from_authorized_user_file(self.config.TOKEN_FILE, ['https://www.googleapis.com/auth/gmail.send'])
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                flow = InstalledAppFlow.from_client_secrets_file(self.config.CREDENTIALS_FILE, ['https://www.googleapis.com/auth/gmail.send'])
                creds = flow.run_local_server(port=0)
            with open(self.config.TOKEN_FILE, 'w') as token:
                token.write(creds.to_json())
        return creds
    def create_email_body(self, news_list, ai_briefing_html, today_date_str):
        env = Environment(loader=FileSystemLoader('.'))
        template = env.get_template('email_template.html')
        return template.render(news_list=news_list, today_date=today_date_str, ai_briefing=ai_briefing_html)
    def send_email(self, subject, body):
        if not self.config.RECIPIENT_LIST:
            logging.warning("âŒ ìˆ˜ì‹ ì ëª©ë¡ì´ ë¹„ì–´ìˆì–´ ì´ë©”ì¼ì„ ë°œì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        try:
            service = build('gmail', 'v1', credentials=self.credentials)
            message = MIMEText(body, 'html', 'utf-8')
            message['To'] = ", ".join(self.config.RECIPIENT_LIST)
            message['From'] = self.config.SENDER_EMAIL
            message['Subject'] = subject
            encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
            create_message = {'raw': encoded_message}
            send_message = service.users().messages().send(userId="me", body=create_message).execute()
            logging.info(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! (Message ID: {send_message['id']})")
        except HttpError as error:
            logging.error("âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨:", exc_info=True)

def main():
    setup_logging()
    logging.info("ğŸš€ ë‰´ìŠ¤ë ˆí„° ìë™ ìƒì„± í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    try:
        config = Config()
        news_scraper = NewsScraper(config)
        ai_service = AIService(config)
        news_service = NewsService(config, news_scraper, ai_service)

        # 1. ë‰´ìŠ¤ í›„ë³´ ìˆ˜ì§‘ ë° ìƒì„¸ ì •ë³´ ì²˜ë¦¬
        all_news = news_service.get_fresh_news()
        if not all_news:
            logging.info("â„¹ï¸ ë°œì†¡í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # 2. AIë¥¼ ì´ìš©í•´ Top 10 ë‰´ìŠ¤ ì„ ë³„
        top_10_news_base = ai_service.select_top_news(all_news)
        if not top_10_news_base:
            logging.warning("âš ï¸ AIê°€ Top ë‰´ìŠ¤ë¥¼ ì„ ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return
            
        # 3. ì„ ë³„ëœ Top 10 ë‰´ìŠ¤ì˜ AI ìš”ì•½ ìƒì„± (API í˜¸ì¶œ ìµœì†Œí™”)
        logging.info(f"âœ… AI Top 10 ì„ ë³„ ì™„ë£Œ. ì„ ë³„ëœ {len(top_10_news_base)}ê°œ ë‰´ìŠ¤ì˜ ê°œë³„ AI ìš”ì•½ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        for news in top_10_news_base:
            news['ai_summary'] = ai_service.generate_single_summary(news['title'], news['full_text'])

        # 4. ì „ì²´ ë¸Œë¦¬í•‘ ìƒì„±
        ai_briefing_md = ai_service.generate_briefing(top_10_news_base)
        ai_briefing_html = markdown_to_html(ai_briefing_md)

        # 5. ì´ë©”ì¼ ë°œì†¡
        email_service = EmailService(config)
        today_str = get_kst_today_str()
        email_subject = f"[{today_str}] ì˜¤ëŠ˜ì˜ í™”ë¬¼/ë¬¼ë¥˜ ë‰´ìŠ¤ Top {len(top_10_news_base)}"
        email_body = email_service.create_email_body(top_10_news_base, ai_briefing_html, today_str)
        email_service.send_email(email_subject, email_body)
        
        # 6. ë°œì†¡ ê¸°ë¡ ì—…ë°ì´íŠ¸
        news_service.update_sent_links_log(top_10_news_base)

        logging.info("ğŸ‰ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except (ValueError, FileNotFoundError) as e:
        logging.critical(f"ğŸš¨ ì„¤ì • ë˜ëŠ” íŒŒì¼ ì˜¤ë¥˜: {e}")
    except Exception as e:
        logging.critical("ğŸ”¥ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ:", exc_info=True)

if __name__ == "__main__":
    main()
