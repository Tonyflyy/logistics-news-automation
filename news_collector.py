# news_collector.py

import os
import base64
import markdown
import json
import time
import random
from datetime import datetime, timezone, timedelta, date
from email.mime.text import MIMEText
from email.mime.image import MIMEImage
from email.mime.multipart import MIMEMultipart
from email.utils import formataddr
from urllib.parse import urljoin, urlparse
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
from newspaper import Article
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from jinja2 import Environment, FileSystemLoader
from PIL import Image
from pygooglenews import GoogleNews
from zoneinfo import ZoneInfo

# â¬‡ï¸â¬‡ï¸â¬‡ï¸ Seleniumì˜ 'ì§€ëŠ¥ì  ê¸°ë‹¤ë¦¼' ê¸°ëŠ¥ì„ ìœ„í•œ ì„í¬íŠ¸ ì¶”ê°€ â¬‡ï¸â¬‡ï¸â¬‡ï¸
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium_stealth import stealth
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# êµ¬ê¸€ ì¸ì¦ ê´€ë ¨
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
import openai

from config import Config

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def get_kst_today_str():
    return datetime.now(ZoneInfo('Asia/Seoul')).strftime("%Y-%m-%d")

def markdown_to_html(text):
    return markdown.markdown(text) if text else ""


def create_price_trend_chart(seven_day_data, filename="price_chart.png"):
    """ìµœê·¼ 7ì¼ê°„ì˜ ìœ ê°€ ë°ì´í„°ë¡œ ì°¨íŠ¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        # 1. í•œê¸€ í°íŠ¸ ì„¤ì • (ë§‘ì€ ê³ ë”•)
        plt.rcParams['font.family'] = 'Malgun Gothic'
        plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ í°íŠ¸ ê¹¨ì§ ë°©ì§€

        # 2. ë°ì´í„° ë¶„ë¦¬ ë° ì¤€ë¹„
        dates = [d['DATE'][-4:-2] + "/" + d['DATE'][-2:] for d in seven_day_data['gasoline']]
        gasoline_prices = [float(p['PRICE']) for p in seven_day_data['gasoline']]
        diesel_prices = [float(p['PRICE']) for p in seven_day_data['diesel']]

        # 3. ì°¨íŠ¸ ìƒì„±
        fig, ax = plt.subplots(figsize=(7, 4)) # ì°¨íŠ¸ í¬ê¸° ì¡°ì ˆ
        
        ax.plot(dates, gasoline_prices, 'o-', label='íœ˜ë°œìœ ', color='#3498db')
        ax.plot(dates, diesel_prices, 'o-', label='ê²½ìœ ', color='#e74c3c')
        
        # 4. ì°¨íŠ¸ ê¾¸ë¯¸ê¸°
        ax.set_title("ìµœê·¼ 7ì¼ íœ˜ë°œìœ Â·ê²½ìœ  ê°€ê²© ì¶”ì´", fontsize=15, pad=20)
        ax.legend()
        ax.grid(True, which='both', linestyle='--', linewidth=0.5)
        
        # Yì¶• ë‹¨ìœ„ë¥¼ '1,700ì›' í˜•ì‹ìœ¼ë¡œ ë³€ê²½
        formatter = FuncFormatter(lambda y, _: f'{int(y):,}ì›')
        ax.yaxis.set_major_formatter(formatter)
        
        ax.tick_params(axis='x', rotation=0)
        fig.tight_layout()

        # 5. ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥
        plt.savefig(filename, dpi=150)
        plt.close(fig) # ë©”ëª¨ë¦¬ í•´ì œ
        
        print(f"âœ… ìœ ê°€ ì¶”ì´ ì°¨íŠ¸ ì´ë¯¸ì§€ '{filename}'ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
        return filename
    except Exception as e:
        print(f"âŒ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    
def get_cheapest_stations(config, count=20):
    """ì˜¤í”¼ë„· APIë¡œ ì „êµ­ ìµœì €ê°€ ê²½ìœ  ì£¼ìœ ì†Œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if not config.OPINET_API_KEY:
        return []

    # API íŒŒë¼ë¯¸í„° ì„¤ì •: prodcd=D047 (ê²½ìœ ), cnt=ê°€ì ¸ì˜¬ ê°œìˆ˜
    url = f"http://www.opinet.co.kr/api/lowTop10.do?out=json&code={config.OPINET_API_KEY}&prodcd=D047&cnt={count}"
    
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        data = response.json()['RESULT']['OIL']
        
        cheapest_stations = []
        for station in data:
            # ì£¼ì†Œì—ì„œ 'ì‹œ/ë„'ì™€ 'ì‹œ/êµ°/êµ¬' ì •ë³´ë§Œ ê°„ì¶”ë¦¬ê¸°
            address_parts = station.get('VAN_ADR', '').split(' ')
            location = " ".join(address_parts[:2]) if len(address_parts) >= 2 else address_parts[0]
            
            cheapest_stations.append({
                "name": station.get('OS_NM'),
                "price": f"{int(station.get('PRICE', 0)):,}ì›",
                "location": location
            })
        
        print(f"âœ… ì „êµ­ ìµœì €ê°€ ì£¼ìœ ì†Œ Top {len(cheapest_stations)} ì •ë³´ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
        return cheapest_stations

    except Exception as e:
        print(f"âŒ ìµœì €ê°€ ì£¼ìœ ì†Œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []
    
def get_price_indicators(config):
    """ì˜¤í”¼ë„· APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ìš” ë„ì‹œë³„ ìœ ê°€, ìš”ì†Œìˆ˜ ê°€ê²©, ì¶”ì„¸, ìµœì €ê°€ ì£¼ìœ ì†Œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ í•˜ë‚˜ì˜ ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not config.OPINET_API_KEY:
        print("âš ï¸ ì˜¤í”¼ë„· API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {}

    # ìµœì¢… ë°ì´í„°ë¥¼ ë‹´ì„ ê¸°ë³¸ êµ¬ì¡° ì •ì˜
    indicator_data = {
        "timestamp": datetime.now(ZoneInfo('Asia/Seoul')).strftime('%Y-%m-%d %H:%M ê¸°ì¤€'),
        "city_prices": [],
        "trend_comment": "",
        "seven_day_data": {},
        "cheapest_stations": []
    }
    
    # --- 1. ì£¼ìš” ë„ì‹œë³„ íœ˜ë°œìœ /ê²½ìœ  ê°€ê²© ê°€ì ¸ì˜¤ê¸° (API í˜¸ì¶œ 1íšŒ) ---
    city_data_map = {code: {"name": name} for code, name in config.AREA_CODE_MAP.items() if code in config.TARGET_AREA_CODES}
    try:
        sido_price_url = f"http://www.opinet.co.kr/api/avgSidoPrice.do?out=json&code={config.OPINET_API_KEY}"
        response = requests.get(sido_price_url, timeout=5)
        response.raise_for_status()
        sido_data = response.json()['RESULT']['OIL']
        for oil in sido_data:
            area_code = oil.get('SIDOCD')
            if area_code in config.TARGET_AREA_CODES:
                prod_code = oil.get('PRODCD')
                price = f"{float(oil['PRICE']):,.0f}ì›"
                if prod_code == 'B027': # ë³´í†µíœ˜ë°œìœ 
                    city_data_map[area_code]['gasoline'] = price
                elif prod_code == 'D047': # ìë™ì°¨ìš©ê²½ìœ 
                    city_data_map[area_code]['diesel'] = price
        print("âœ… ì£¼ìš” ë„ì‹œë³„ ìœ ê°€ ì •ë³´ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì‹œë„ë³„ ìœ ê°€ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # --- 2. ì£¼ìš” ë„ì‹œë³„ ìš”ì†Œìˆ˜ í‰ê·  ê°€ê²© ê°€ì ¸ì˜¤ê¸° (ë„ì‹œë³„ API í˜¸ì¶œ) ---
    print("-> ì£¼ìš” ë„ì‹œë³„ ìš”ì†Œìˆ˜ ê°€ê²© ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤...")
    for area_code in config.TARGET_AREA_CODES:
        urea_url = f"http://www.opinet.co.kr/api/ureaPrice.do?out=json&code={config.OPINET_API_KEY}&area={area_code}"
        try:
            response = requests.get(urea_url, timeout=5)
            response.raise_for_status()
            urea_data = json.loads(response.text, strict=False)['RESULT']['OIL']
            total_price, stock_count = 0, 0
            for station in urea_data:
                stock_yn = station.get('STOCK_YN', '').strip()
                price_str = station.get('PRICE', '').strip()
                if stock_yn == 'Y' and price_str:
                    total_price += int(price_str)
                    stock_count += 1
            if stock_count > 0:
                avg_price = total_price / stock_count
                city_data_map[area_code]['urea'] = f"{avg_price:,.0f}ì›/L"
            time.sleep(0.5)
        except Exception as e:
            area_name = config.AREA_CODE_MAP.get(area_code, "ì•Œ ìˆ˜ ì—†ëŠ” ì§€ì—­")
            print(f"âŒ {area_name} ìš”ì†Œìˆ˜ ê°€ê²© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            continue
    print("âœ… ì£¼ìš” ë„ì‹œë³„ ìš”ì†Œìˆ˜ ê°€ê²© ì •ë³´ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

    # --- 3. ì „êµ­ ê°€ê²© ì¶”ì„¸ ë° ì°¨íŠ¸ìš© ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (API í˜¸ì¶œ 1íšŒ) ---
    try:
        trend_url = f"http://www.opinet.co.kr/api/avgRecentPrice.do?out=json&code={config.OPINET_API_KEY}"
        response = requests.get(trend_url, timeout=5)
        response.raise_for_status()
        trend_data = response.json()['RESULT']['OIL']
        
        # ì°¨íŠ¸ìš© 7ì¼ ë°ì´í„° ì¤€ë¹„
        gasoline_7day = sorted([p for p in trend_data if p['PRODCD'] == 'B027'], key=lambda x: x['DATE'])
        diesel_7day = sorted([p for p in trend_data if p['PRODCD'] == 'D047'], key=lambda x: x['DATE'])
        if gasoline_7day and diesel_7day:
            indicator_data["seven_day_data"] = {"gasoline": gasoline_7day, "diesel": diesel_7day}
            print("âœ… ì°¨íŠ¸ìš© 7ì¼ ìœ ê°€ ë°ì´í„°ë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.")

        # ê²½ìœ  ê°€ê²© ì¶”ì„¸ ë¶„ì„
        if len(diesel_7day) >= 2:
            today_price = float(diesel_7day[-1]['PRICE'])
            yesterday_price = float(diesel_7day[-2]['PRICE'])
            trend_comment = ""
            if today_price > yesterday_price: trend_comment += "ì–´ì œë³´ë‹¤ ì†Œí­ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤."
            elif today_price < yesterday_price: trend_comment += "ì–´ì œë³´ë‹¤ ì†Œí­ í•˜ë½í–ˆìŠµë‹ˆë‹¤."
            else: trend_comment += "ì–´ì œì™€ ê°€ê²©ì´ ë™ì¼í•©ë‹ˆë‹¤."
            
            if len(diesel_7day) >= 7:
                week_ago_price = float(diesel_7day[0]['PRICE'])
                if today_price > week_ago_price: trend_comment += " ì£¼ê°„ ë‹¨ìœ„ë¡œëŠ” ìƒìŠ¹ì„¸ì…ë‹ˆë‹¤."
                elif today_price < week_ago_price: trend_comment += " ì£¼ê°„ ë‹¨ìœ„ë¡œëŠ” í•˜ë½ì„¸ì…ë‹ˆë‹¤."
            
            indicator_data["trend_comment"] = f"ì „êµ­ ê²½ìœ  ê°€ê²©ì€ {trend_comment}"
            print("âœ… ì „êµ­ ìœ ê°€ ì¶”ì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ìœ ê°€ ì¶”ì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    # --- 4. ì „êµ­ ìµœì €ê°€ ì£¼ìœ ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ---
    indicator_data["cheapest_stations"] = get_cheapest_stations(config, count=20)

    # --- ìµœì¢… ë°ì´í„° êµ¬ì¡° ì •ë¦¬ ---
    indicator_data["city_prices"] = list(city_data_map.values())
    return indicator_data
    

class NewsScraper:
    def __init__(self, config):
        self.config = config
        self.session = self._create_session()

    def _create_session(self):
        session = requests.Session()
        retry = Retry(total=2, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        return session

    def get_image_url(self, article_url: str) -> str:
        try:
            headers = { "User-Agent": random.choice(self.config.USER_AGENTS) }
            response = self.session.get(article_url, headers=headers, timeout=10, allow_redirects=True)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            
            # 1. ë©”íƒ€ íƒœê·¸ (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ)
            meta_image = soup.find("meta", property="og:image") or soup.find("meta", name="twitter:image")
            if meta_image and meta_image.get("content"):
                meta_url = self._resolve_url(article_url, meta_image["content"])
                if self._is_valid_candidate(meta_url) and self._validate_image(meta_url):
                    return meta_url

            # 2. ë³¸ë¬¸ <figure> ë˜ëŠ” <picture> íƒœê·¸
            for tag in soup.select('figure > img, picture > img', limit=5):
                img_url = tag.get('src') or tag.get('data-src') or (tag.get('srcset').split(',')[0].strip().split(' ')[0] if tag.get('srcset') else None)
                if img_url and self._is_valid_candidate(img_url):
                    full_url = self._resolve_url(article_url, img_url)
                    if self._validate_image(full_url):
                        return full_url
            
            # 2.5. ê¸°ì‚¬ ë³¸ë¬¸ ì˜ì—­(entry-content, article-body ë“±)ì„ íŠ¹ì •í•˜ì—¬ ì´ë¯¸ì§€ ê²€ìƒ‰
            content_area = soup.select_one('.entry-content, .article-body, #article-view-content')
            if content_area:
                for img in content_area.find_all("img", limit=5):
                    img_url = img.get("src") or img.get("data-src")
                    if img_url and self._is_valid_candidate(img_url):
                        full_url = self._resolve_url(article_url, img_url)
                        if self._validate_image(full_url):
                            return full_url
            # --- â¬†ï¸â¬†ï¸â¬†ï¸ ìˆ˜ì • ì™„ë£Œ â¬†ï¸â¬†ï¸â¬†ï¸
            
            # 3. ì¼ë°˜ <img> íƒœê·¸ (ìµœí›„ì˜ ìˆ˜ë‹¨)
            for img in soup.find_all("img", limit=10):
                img_url = img.get("src") or img.get("data-src")
                if img_url and self._is_valid_candidate(img_url):
                    full_url = self._resolve_url(article_url, img_url)
                    if self._validate_image(full_url):
                        return full_url

            return self.config.DEFAULT_IMAGE_URL
        except Exception:
            return self.config.DEFAULT_IMAGE_URL

    def _resolve_url(self, base_url, image_url):
        if image_url.startswith('//'): return 'https:' + image_url
        return urljoin(base_url, image_url)

    def _is_valid_candidate(self, image_url):
        if 'news.google.com' in image_url or 'lh3.googleusercontent.com' in image_url: return False
        return not any(pattern in image_url.lower() for pattern in self.config.UNWANTED_IMAGE_PATTERNS)

    def _validate_image(self, image_url):
        try:
            response = self.session.get(image_url, stream=True, timeout=5)
            response.raise_for_status()
            content_type = response.headers.get('Content-Type', '').lower()
            if 'image' not in content_type: return False
            img_data = BytesIO(response.content)
            with Image.open(img_data) as img:
                width, height = img.size
                if width < self.config.MIN_IMAGE_WIDTH or height < self.config.MIN_IMAGE_HEIGHT: return False
                aspect_ratio = width / height
                if aspect_ratio > 4.0 or aspect_ratio < 0.25: return False
                

                return True
        except Exception:
            return False

class AIService:
    def generate_single_summary(self, article_title: str, article_link: str) -> str | None:
        """ê¸°ì‚¬ ì œëª©ê³¼ ì›ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ 3ì¤„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            article = Article(article_link)
            article.download()
            article.parse()
            
            if len(article.text) < 100:
                return "ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            system_prompt = "ë‹¹ì‹ ì€ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ëŠ” ë‰´ìŠ¤ ì—ë””í„°ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤."
            user_prompt = f"""
            ì•„ë˜ ì œëª©ê³¼ ë³¸ë¬¸ì„ ê°€ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ë…ìë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ 3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
            
            [ì œëª©]: {article_title}
            [ë³¸ë¬¸]:
            {article.text[:2000]}
            """
            
            summary = self._generate_content_with_retry(system_prompt, user_prompt)
            return summary

        except Exception as e:
            print(f" Â ã„´> âŒ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e.__class__.__name__}")
            return None
    # (ë³€ê²½ ì—†ìŒ)
    def __init__(self, config):
        self.config = config
        if not self.config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)

    def _generate_content_with_retry(self, system_prompt: str, user_prompt: str, is_json: bool = False):
        """
        OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
        - system_prompt: AIì˜ ì—­í• ê³¼ ì§€ì¹¨ì„ ì •ì˜í•©ë‹ˆë‹¤.
        - user_prompt: AIì—ê²Œ ì „ë‹¬í•  ì‹¤ì œ ìš”ì²­ ë‚´ìš©ì…ë‹ˆë‹¤.
        - is_json: JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ìš”ì²­í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # JSON ëª¨ë“œ ìš”ì²­ ì‹œ ì¶”ê°€ ì˜µì…˜ ì„¤ì •
        request_options = {"model": self.config.GPT_MODEL, "messages": messages}
        if is_json:
            request_options["response_format"] = {"type": "json_object"}

        for attempt in range(3):
            try:
                response = self.client.chat.completions.create(**request_options)
                content = response.choices[0].message.content
                
                # JSON ëª¨ë“œì¼ ê²½ìš°, ì‘ë‹µì´ ìœ íš¨í•œ JSONì¸ì§€ í•œ ë²ˆ ë” í™•ì¸
                if is_json:
                    json.loads(content) # íŒŒì‹±ì— ì‹¤íŒ¨í•˜ë©´ ì˜ˆì™¸ ë°œìƒ
                
                return content
            
            except Exception as e:
                print(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3): {e}")
                time.sleep(2 ** attempt) # ì¬ì‹œë„ ì „ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        return None

    def select_top_news(self, news_list, previous_news_list):
        """
        ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ê³  ê°€ì¥ ì¤‘ìš”í•œ Top 10 ë‰´ìŠ¤ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.
        - news_list: ì˜¤ëŠ˜ì˜ í›„ë³´ ë‰´ìŠ¤ ëª©ë¡
        - previous_news_list: ì–´ì œ ë°œì†¡í–ˆë˜ ë‰´ìŠ¤ ëª©ë¡
        """
        print(f"AI ë‰´ìŠ¤ ì„ ë³„ ì‹œì‘... (ëŒ€ìƒ: {len(news_list)}ê°œ)")

        # (ì¶”ê°€) ì–´ì œ ë‰´ìŠ¤ ëª©ë¡ì„ AIì—ê²Œ ì „ë‹¬í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        previous_news_context = "ì–´ì œëŠ” ë°œì†¡ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
        if previous_news_list:
            previous_news_context = "\n\n".join(
                [f"- ì œëª©: {news['title']}\n  ìš”ì•½: {news['ai_summary']}" for news in previous_news_list]
            )

        # ì˜¤ëŠ˜ì˜ í›„ë³´ ë‰´ìŠ¤ ëª©ë¡ì„ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        today_candidates_context = "\n\n".join(
            [f"ê¸°ì‚¬ #{i}\nì œëª©: {news['title']}\nìš”ì•½: {news['ai_summary']}" for i, news in enumerate(news_list)]
        )

        system_prompt = "ë‹¹ì‹ ì€ ë…ìì—ê²Œ ë§¤ì¼ ì‹ ì„ í•˜ê³  ê°€ì¹˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ìµœìš°ì„ ìœ¼ë¡œ í•˜ëŠ” ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ë¬¼ë¥˜ ì „ë¬¸ ë‰´ìŠ¤ í¸ì§‘ì¥ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤."
        
        # (ë³€ê²½) ë‘ ê°€ì§€ ì¤‘ë³µ ì œê±° ê·œì¹™ì´ ëª¨ë‘ í¬í•¨ëœ ìµœì¢… í”„ë¡¬í”„íŠ¸
        user_prompt = f"""
        [ì–´ì œ ë°œì†¡ëœ ì£¼ìš” ë‰´ìŠ¤]
        {previous_news_context}

        ---

        [ì˜¤ëŠ˜ì˜ í›„ë³´ ë‰´ìŠ¤ ëª©ë¡]
        {today_candidates_context}

        ---

        [ë‹¹ì‹ ì˜ ê°€ì¥ ì¤‘ìš”í•œ ì„ë¬´ì™€ ê·œì¹™]
        1.  **ìƒˆë¡œìš´ ì£¼ì œ ìµœìš°ì„ **: [ì˜¤ëŠ˜ì˜ í›„ë³´ ë‰´ìŠ¤ ëª©ë¡]ì—ì„œ ë‰´ìŠ¤ë¥¼ ì„ íƒí•  ë•Œ, [ì–´ì œ ë°œì†¡ëœ ì£¼ìš” ë‰´ìŠ¤]ì™€ **ì£¼ì œê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ì†Œì‹**ì„ ìµœìš°ì„ ìœ¼ë¡œ ì„ ì •í•´ì•¼ í•©ë‹ˆë‹¤.
        2.  **ì¤‘ìš” í›„ì† ê¸°ì‚¬ë§Œ í—ˆìš©**: ì–´ì œ ë‰´ìŠ¤ì˜ í›„ì† ê¸°ì‚¬ëŠ” 'ê³„íš ë°œí‘œ'ì—ì„œ 'ì •ì‹ ê³„ì•½ ì²´ê²°'ì²˜ëŸ¼ **ë§¤ìš° ì¤‘ëŒ€í•œ ì§„ì „ì´ ìˆì„ ê²½ìš°ì—ë§Œ** í¬í•¨ì‹œí‚¤ê³ , ë‹¨ìˆœ ì§„í–‰ ìƒí™© ë³´ë„ëŠ” ê³¼ê°íˆ ì œì™¸í•˜ì„¸ìš”.
        3.  **ì˜¤ëŠ˜ ë‰´ìŠ¤ ë‚´ ì¤‘ë³µ ì œê±°**: [ì˜¤ëŠ˜ì˜ í›„ë³´ ë‰´ìŠ¤ ëª©ë¡] ë‚´ì—ì„œë„ ë™ì¼í•œ ì‚¬ê±´(ì˜ˆ: 'Aì‚¬ ë¬¼ë¥˜ì„¼í„° ê°œì¥')ì„ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ê°€ ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ì—ì„œ ë‚˜ì™”ë‹¤ë©´, ê°€ì¥ ì œëª©ì´ êµ¬ì²´ì ì´ê³  ë‚´ìš©ì´ í’ë¶€í•œ **ê¸°ì‚¬ ë‹¨ í•˜ë‚˜ë§Œ**ì„ ëŒ€í‘œë¡œ ì„ ì •í•´ì•¼ í•©ë‹ˆë‹¤.

        [ì‘ì—… ì§€ì‹œ]
        ìœ„ì˜ ê·œì¹™ë“¤ì„ ê°€ì¥ ì—„ê²©í•˜ê²Œ ì¤€ìˆ˜í•˜ì—¬, [ì˜¤ëŠ˜ì˜ í›„ë³´ ë‰´ìŠ¤ ëª©ë¡] ì¤‘ì—ì„œ ë…ìì—ê²Œ ê°€ì¥ ê°€ì¹˜ìˆëŠ” ìµœì¢… ê¸°ì‚¬ 10ê°œì˜ ë²ˆí˜¸(ì¸ë±ìŠ¤)ë¥¼ ì„ ì •í•´ì£¼ì„¸ìš”.

        [ì¶œë ¥ í˜•ì‹]
        - ë°˜ë“œì‹œ 'selected_indices' í‚¤ì— ìµœì¢… ì„ ì •í•œ ê¸°ì‚¬ 10ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ìˆ«ì ë°°ì—´ë¡œ ë‹´ì€ JSON ê°ì²´ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
        - ì˜ˆ: {{"selected_indices": [3, 15, 4, 8, 22, 1, 30, 11, 19, 5]}}
        """
        
        response_text = self._generate_content_with_retry(system_prompt, user_prompt, is_json=True)
        
        if response_text:
            try:
                selected_indices = json.loads(response_text).get('selected_indices', [])
                top_news = [news_list[i] for i in selected_indices if i < len(news_list)]
                print(f"âœ… AIê°€ {len(top_news)}ê°œ ë‰´ìŠ¤ë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.")
                return top_news
            except (json.JSONDecodeError, KeyError) as e:
                print(f"âŒ AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}. ìƒìœ„ 10ê°œ ë‰´ìŠ¤ë¥¼ ì„ì˜ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")
        
        return news_list[:10]

    def generate_briefing(self, news_list):
        """ì„ ë³„ëœ ë‰´ìŠ¤ ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        print("AI ë¸Œë¦¬í•‘ ìƒì„± ì‹œì‘...")
        context = "\n\n".join([f"ì œëª©: {news['title']}\nìš”ì•½: {news['ai_summary']}" for news in news_list])
        
        system_prompt = "ë‹¹ì‹ ì€ íƒì›”í•œ í†µì°°ë ¥ì„ ê°€ì§„ IT/ê²½ì œ ë‰´ìŠ¤ íë ˆì´í„°ì…ë‹ˆë‹¤. Markdown í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë§¤ìš° ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ 'ë°ì¼ë¦¬ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
        user_prompt = f"""
        ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, ë…ìë¥¼ ìœ„í•œ 'ë°ì¼ë¦¬ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        **ì¶œë ¥ í˜•ì‹ ê·œì¹™:**
        1. 'ì—ë””í„° ë¸Œë¦¬í•‘'ì€ '## ì—ë””í„° ë¸Œë¦¬í•‘' í—¤ë”ë¡œ ì‹œì‘í•˜ë©°, ì˜¤ëŠ˜ ë‰´ìŠ¤ì˜ í•µì‹¬ì„ 2~3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
        2. 'ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„'ì€ '## ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„' í—¤ë”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        3. ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„ì—ì„œëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ 2~3ê°œë¥¼ '###' í—¤ë”ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
        4. ê° ì¹´í…Œê³ ë¦¬ ì•ˆì—ì„œëŠ”, ê´€ë ¨ëœ ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³  ê¸€ë¨¸ë¦¬ ê¸°í˜¸(`*`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        5. ë¬¸ì¥ ì•ˆì—ì„œ ê°•ì¡°í•˜ê³  ì‹¶ì€ íŠ¹ì • í‚¤ì›Œë“œëŠ” í°ë”°ì˜´í‘œ(" ")ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”.
        
        [ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ëª©ë¡]
        {context}
        """
        
        briefing = self._generate_content_with_retry(system_prompt, user_prompt)
        if briefing: 
            print("âœ… AI ë¸Œë¦¬í•‘ ìƒì„± ì„±ê³µ!")
        return briefing


class NewsService:
    def __init__(self, config, scraper, ai_service):
        self.config = config
        self.scraper = scraper
        self.ai_service = ai_service
        self.sent_links = self._load_sent_links()

    def _create_stealth_driver(self):
        chrome_options = Options()
        chrome_options.page_load_strategy = 'eager'
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument(f'--user-agent={random.choice(self.config.USER_AGENTS)}')
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        
        try:
            service = ChromeService(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            stealth(driver, languages=["ko-KR", "ko"], vendor="Google Inc.", platform="Win32",
                    webgl_vendor="Intel Inc.", renderer="Intel Iris OpenGL Engine", fix_hairline=True)
            driver.set_page_load_timeout(15)
            return driver
        except Exception as e:
            print(f"ğŸš¨ ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _load_sent_links(self):
        try:
            with open(self.config.SENT_LINKS_FILE, 'r', encoding='utf-8') as f:
                return set(line.strip() for line in f)
        except FileNotFoundError:
            return set()

    def _clean_and_validate_url(self, url: str) -> str | None:
        try:
            parsed = urlparse(url)
            if any(ad_domain in parsed.netloc for ad_domain in self.config.AD_DOMAINS_BLACKLIST):
                return None
            
            if not parsed.path or len(parsed.path) <= 5:
                if not any(allowed in parsed.netloc for allowed in ['hyundai.co.kr']):
                    return None
            
            cleaned_url = parsed._replace(fragment="").geturl()
            return cleaned_url
        except Exception:
            return None
    
    def _resolve_google_news_url(self, entry):
        """Seleniumì„ ì‚¬ìš©í•´ Google News ë§í¬ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URLë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        driver = None
        try:
            driver = self._create_stealth_driver()
            if not driver: return None
            
            driver.get(entry['link'])
            wait = WebDriverWait(driver, 10)
            link_element = wait.until(EC.presence_of_element_located((By.TAG_NAME, 'a')))
            original_url = link_element.get_attribute('href')
            validated_url = self._clean_and_validate_url(original_url)
            
            if validated_url:
                print(f" Â -> âœ… URL ì¶”ì¶œ ì„±ê³µ: {entry['title']}")
                return {'title': entry['title'], 'link': validated_url}
            return None
        except Exception as e:
            print(f" Â ã„´> âŒ URL ì¶”ì¶œ ì‹¤íŒ¨: '{entry['title']}'ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e.__class__.__name__}")
            return None
        finally:
            if driver:
                driver.quit()

    def _process_article_content(self, article_info):
        """ì‹¤ì œ URLì„ ë°›ì•„ ì½˜í…ì¸  ë¶„ì„, AI ìš”ì•½, ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        title = article_info['title']
        url = article_info['link']
        
        try:
            headers = {"User-Agent": random.choice(self.config.USER_AGENTS)}
            response = self.scraper.session.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            article_text = ''
            content_area = soup.select_one('#article-view-content, .article_body, .article-body, .entry-content')
            if content_area:
                article_text = content_area.get_text(strip=True)
            else:
                article_text = ' '.join(p.get_text(strip=True) for p in soup.find_all('p'))

            if len(article_text) < 300:
                print(f" Â ã„´> ğŸ—‘ï¸ ë³¸ë¬¸ ë‚´ìš©ì´ ì§§ì•„ ì œì™¸: {url[:80]}...")
                return None

            ai_summary = self.ai_service.generate_single_summary(title, url)
            if not ai_summary or "ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in ai_summary:
                print(f" Â ã„´> âš ï¸ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨, ê¸°ì‚¬ ì œì™¸")
                return None
            
            return {
                'title': title,
                'link': url, 'url': url,
                'ai_summary': ai_summary,
                'image_url': self.scraper.get_image_url(url)
            }
        except Exception as e:
            print(f" Â ã„´> âŒ ì½˜í…ì¸  ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: '{title}' ({e.__class__.__name__})")
            return None

    def get_fresh_news(self):
        print("ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        client = GoogleNews(lang='ko', country='KR')
        
        # --- â¬‡ï¸ (ë³€ê²½) ê·¸ë£¹ ê²€ìƒ‰ ë¡œì§ ì‹œì‘ â¬‡ï¸ ---
        all_entries = []
        unique_links = set() # ë§í¬ ì¤‘ë³µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ê¸° ìœ„í•œ set

        # ê²€ìƒ‰í•  ê¸°ê°„ ì„¤ì •
        end_date = date.today()
        start_date = end_date - timedelta(hours=self.config.NEWS_FETCH_HOURS)
        
        print(f"ê²€ìƒ‰ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")

        # ì„¤ì •ëœ í‚¤ì›Œë“œ ê·¸ë£¹ì„ í•˜ë‚˜ì”© ìˆœíšŒ
        for i, group in enumerate(self.config.KEYWORD_GROUPS):
            query = ' OR '.join(f'"{keyword}"' for keyword in group) # í‚¤ì›Œë“œì— ê³µë°±ì´ ìˆì–´ë„ ì•ˆì „í•˜ë„ë¡ "" ì²˜ë¦¬
            query += ' -í•´ìš´ -í•­ê³µ' # ì œì™¸ í‚¤ì›Œë“œ ì¶”ê°€
            
            print(f"\n({i+1}/{len(self.config.KEYWORD_GROUPS)}) ê·¸ë£¹ ê²€ìƒ‰ ì¤‘: [{', '.join(group)}]")

            try:
                # ê° ê·¸ë£¹ë³„ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰
                search_results = client.search(query, from_=start_date.strftime('%Y-%m-%d'), to_=end_date.strftime('%Y-%m-%d'))
                
                # ì¤‘ë³µì„ í™•ì¸í•˜ë©° ê²°ê³¼ ìˆ˜ì§‘
                for entry in search_results['entries']:
                    link = entry.get('link')
                    if link and link not in unique_links:
                        all_entries.append(entry)
                        unique_links.add(link)
                
                print(f" â¡ï¸ {len(search_results['entries'])}ê°œ ë°œê²¬, í˜„ì¬ê¹Œì§€ ì´ {len(all_entries)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ í™•ë³´")

                # IP ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•´ ê° ìš”ì²­ ì‚¬ì´ì— 2ì´ˆ ëŒ€ê¸°
                time.sleep(2)

            except Exception as e:
                print(f" âŒ ê·¸ë£¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print(f"\nëª¨ë“  ê·¸ë£¹ ê²€ìƒ‰ ì™„ë£Œ. ì´ {len(all_entries)}ê°œì˜ ì¤‘ë³µ ì—†ëŠ” ê¸°ì‚¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        # --- â¬†ï¸ (ë³€ê²½) ê·¸ë£¹ ê²€ìƒ‰ ë¡œì§ ì¢…ë£Œ â¬†ï¸ ---

        # ì‹œê°„ í•„í„°ë§ (ì´ë¯¸ ê²€ìƒ‰ ì‹œ ê¸°ê°„ì„ ì •í–ˆì§€ë§Œ, ë” ì •í™•í•˜ê²Œ ì‹œê°„ ë‹¨ìœ„ë¡œ í•„í„°ë§)
        valid_articles = []
        now = datetime.now(timezone.utc)
        time_limit = timedelta(hours=self.config.NEWS_FETCH_HOURS)

        for entry in all_entries:
            if 'published_parsed' in entry:
                published_dt = datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc)
                if (now - published_dt) <= time_limit:
                    valid_articles.append(entry)
        
        print(f"ì‹œê°„ í•„í„°ë§ í›„ {len(valid_articles)}ê°œì˜ ìœ íš¨í•œ ê¸°ì‚¬ê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤.")
        
        # ì´ë¯¸ ë°œì†¡ëœ ë§í¬ ì œì™¸
        new_articles = [article for article in valid_articles if self._clean_and_validate_url(article['link']) not in self.sent_links]
        print(f"ì´ë¯¸ ë°œì†¡ëœ ê¸°ì‚¬ë¥¼ ì œì™¸í•˜ê³ , ì´ {len(new_articles)}ê°œì˜ ìƒˆë¡œìš´ í›„ë³´ ê¸°ì‚¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        if not new_articles:
            print("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # --- ë‚˜ë¨¸ì§€ ë¡œì§ì€ ê¸°ì¡´ê³¼ ê±°ì˜ ë™ì¼ ---
        print("\n--- 1ë‹¨ê³„: ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬) ---")
        resolved_articles = []
        with ThreadPoolExecutor(max_workers=5) as executor: # URL ì¶”ì¶œë„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ ì†ë„ ê°œì„ 
            future_to_entry = {executor.submit(self._resolve_google_news_url, entry): entry for entry in new_articles[:self.config.MAX_ARTICLES]}
            for future in as_completed(future_to_entry):
                resolved_info = future.result()
                if resolved_info:
                    resolved_articles.append(resolved_info)
        print(f"--- 1ë‹¨ê³„ ì™„ë£Œ: {len(resolved_articles)}ê°œì˜ ìœ íš¨í•œ ì‹¤ì œ URL í™•ë³´ ---\n")

        if not resolved_articles:
            print("URL ì¶”ì¶œ í›„ ì²˜ë¦¬í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"--- 2ë‹¨ê³„: ê¸°ì‚¬ ì½˜í…ì¸  ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ëŒ€ìƒ: {len(resolved_articles)}ê°œ) ---")
        processed_news = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_article = {executor.submit(self._process_article_content, article): article for article in resolved_articles}
            for future in as_completed(future_to_article):
                article = future_to_article[future]
                try:
                    result = future.result(timeout=60)
                    if result:
                        processed_news.append(result)
                except TimeoutError:
                    print(f" Â ã„´> âŒ ì‹œê°„ ì´ˆê³¼: '{article['title']}' ê¸°ì‚¬ ì²˜ë¦¬ê°€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ ê±´ë„ˆëœë‹ˆë‹¤.")
                except Exception as exc:
                    print(f" Â ã„´> âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: '{article['title']}' ê¸°ì‚¬ì—ì„œ ì˜ˆì™¸ ë°œìƒ: {exc}")
        
        print(f"--- 2ë‹¨ê³„ ì™„ë£Œ: ì´ {len(processed_news)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì„±ê³µ ---\n")
        return processed_news

    def update_sent_links_log(self, news_list):
        links = [news['link'] for news in news_list]
        try:
            with open(self.config.SENT_LINKS_FILE, 'a', encoding='utf-8') as f:
                for link in links: f.write(link + '\n')
            print(f"âœ… {len(links)}ê°œ ë§í¬ë¥¼ ë°œì†¡ ê¸°ë¡ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ë°œì†¡ ê¸°ë¡ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

class EmailService:
    def __init__(self, config):
        self.config = config
        self.credentials = self._get_credentials()

    def _get_credentials(self):
        creds = None
        if os.path.exists(self.config.TOKEN_FILE):
            creds = Credentials.from_authorized_user_file(self.config.TOKEN_FILE, ['https://www.googleapis.com/auth/gmail.send'])
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                flow = InstalledAppFlow.from_client_secrets_file(self.config.CREDENTIALS_FILE, ['https://www.googleapis.com/auth/gmail.send'])
                creds = flow.run_local_server(port=0)
            with open(self.config.TOKEN_FILE, 'w') as token:
                token.write(creds.to_json())
        return creds

    def create_email_body(self, news_list, ai_briefing_html, today_date_str, price_indicators):
        env = Environment(loader=FileSystemLoader('.'))
        template = env.get_template('email_template.html')

        return template.render(
            news_list=news_list, 
            today_date=today_date_str, 
            ai_briefing=ai_briefing_html, 
            price_indicators = price_indicators
        )

    def send_email(self, subject, body_html, image_path=None):
        if not self.config.RECIPIENT_LIST:
            print("âŒ ìˆ˜ì‹ ì ëª©ë¡ì´ ë¹„ì–´ìˆì–´ ì´ë©”ì¼ì„ ë°œì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        try:
            service = build('gmail', 'v1', credentials=self.credentials)
        
            # ì´ë©”ì¼ ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ë³´ë‚´ê¸° ìœ„í•œ MIMEMultipart ê°ì²´ ìƒì„±
            message = MIMEMultipart('related')
            message['To'] = ", ".join(self.config.RECIPIENT_LIST)
            message['From'] = formataddr((self.config.SENDER_NAME, self.config.SENDER_EMAIL))
            message['Subject'] = subject

            # HTML ë³¸ë¬¸ ì²¨ë¶€
            msg_alternative = MIMEMultipart('alternative')
            msg_alternative.attach(MIMEText(body_html, 'html', 'utf-8'))
            message.attach(msg_alternative)

            # ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆìœ¼ë©´ ì²¨ë¶€
            if image_path and os.path.exists(image_path):
                with open(image_path, 'rb') as f:
                    msg_image = MIMEImage(f.read())
                    # Content-ID ì„¤ì •. HTMLì˜ <img src="cid:price_chart">ì—ì„œ ì´ IDë¥¼ ì‚¬ìš©í•¨
                    msg_image.add_header('Content-ID', '<price_chart>')
                    message.attach(msg_image)
                    print(f"âœ… ì´ë©”ì¼ì— '{image_path}' ì´ë¯¸ì§€ë¥¼ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤.")

            encoded_message = base64.urlsafe_b64encode(message.as_bytes()).decode()
            create_message = {'raw': encoded_message}
            send_message = service.users().messages().send(userId="me", body=create_message).execute()
            print(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! (Message ID: {send_message['id']})")
        except HttpError as error:
            print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨: {error}")

def load_newsletter_history(filepath='previous_newsletter.json'):
    """ì´ì „ì— ë°œì†¡ëœ ë‰´ìŠ¤ë ˆí„° ë‚´ìš©ì„ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            history = json.load(f)
            print(f"âœ… ì´ì „ ë‰´ìŠ¤ë ˆí„° ê¸°ë¡({len(history)}ê°œ)ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            return history
    except FileNotFoundError:
        print("â„¹ï¸ ì´ì „ ë‰´ìŠ¤ë ˆí„° ê¸°ë¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì²« ì‹¤í–‰ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
        return []
    except Exception as e:
        print(f"âŒ ì´ì „ ë‰´ìŠ¤ë ˆí„° ê¸°ë¡ ë¡œë”© ì‹¤íŒ¨: {e}")
        return []

def save_newsletter_history(news_list, filepath='previous_newsletter.json'):
    """ë°œì†¡ ì™„ë£Œëœ ë‰´ìŠ¤ë ˆí„° ë‚´ìš©ì„ ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=4)
        print(f"âœ… ì´ë²ˆ ë‰´ìŠ¤ë ˆí„° ë‚´ìš©({len(news_list)}ê°œ)ì„ ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ë ˆí„° ë‚´ìš© ì €ì¥ ì‹¤íŒ¨: {e}")

def main():
    print("ğŸš€ ë‰´ìŠ¤ë ˆí„° ìë™ ìƒì„± í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    try:
        config = Config()
        news_scraper = NewsScraper(config)
        ai_service = AIService(config)
        news_service = NewsService(config, news_scraper, ai_service)
        email_service = EmailService(config)

        # 1. ì´ì „ ë‰´ìŠ¤ ê¸°ë¡ ë° ëª¨ë“  ê°€ê²© ì§€í‘œ ê°€ì ¸ì˜¤ê¸°
        previous_top_news = load_newsletter_history()
        price_indicators = get_price_indicators(config)

        # 2. (ì¶”ê°€) ìœ ê°€ ë°ì´í„°ë¡œ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„±
        chart_image_file = None
        if price_indicators.get("seven_day_data"):
            chart_image_file = create_price_trend_chart(price_indicators["seven_day_data"])

        # 3. ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ì„ ë³„
        all_news = news_service.get_fresh_news()
        if not all_news:
            print("â„¹ï¸ ë°œì†¡í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        top_news = ai_service.select_top_news(all_news, previous_top_news)
        if not top_news:
            print("â„¹ï¸ AIê°€ ë‰´ìŠ¤ë¥¼ ì„ ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # 4. AI ë¸Œë¦¬í•‘ ìƒì„± ë° ì´ë©”ì¼ ë³¸ë¬¸ ì¤€ë¹„
        ai_briefing_md = ai_service.generate_briefing(top_news)
        ai_briefing_html = markdown_to_html(ai_briefing_md)
        today_str = get_kst_today_str()
        email_subject = f"[{today_str}] ì˜¤ëŠ˜ì˜ í™”ë¬¼/ë¬¼ë¥˜ ë‰´ìŠ¤ Top {len(top_news)}"
        
        email_body = email_service.create_email_body(top_news, ai_briefing_html, today_str, price_indicators)
        
        # 5. (ì¶”ê°€) ì´ë©”ì¼ ë°œì†¡ ì‹œ ìƒì„±ëœ ì°¨íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
        email_service.send_email(email_subject, email_body, chart_image_file)
        
        # 6. ë¡œê·¸ ë° íˆìŠ¤í† ë¦¬ ì €ì¥
        news_service.update_sent_links_log(top_news)
        save_newsletter_history(top_news)

        print("\nğŸ‰ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except (ValueError, FileNotFoundError) as e:
        print(f"ğŸš¨ ì„¤ì • ë˜ëŠ” íŒŒì¼ ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"ğŸ”¥ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e.__class__.__name__}: {e}")


if __name__ == "__main__":
    main()
