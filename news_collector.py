import os, base64, markdown, json, time, random, re, logging
from datetime import datetime
from email.mime.text import MIMEText
from urllib.parse import urlparse, urljoin
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from email.mime.multipart import MIMEMultipart
from email.mime.image import MIMEImage

import openai
# ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import ssl
from bs4 import BeautifulSoup
from jinja2 import Environment, FileSystemLoader
from PIL import Image
from zoneinfo import ZoneInfo
from newspaper import Article, ArticleException
from newspaper.article import ArticleDownloadState

# êµ¬ê¸€ ì¸ì¦ ê´€ë ¨
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
from config import Config

class CustomHttpAdapter(HTTPAdapter):
    def __init__(self, *args, **kwargs):
        self.ssl_context = ssl.create_default_context()
        self.ssl_context.set_ciphers('DEFAULT@SECLEVEL=1')
        super().__init__(*args, **kwargs)

    def init_poolmanager(self, connections, maxsize, block=False):
        self.poolmanager = requests.packages.urllib3.poolmanager.PoolManager(
            num_pools=connections, maxsize=maxsize,
            block=block, ssl_context=self.ssl_context)

def setup_logging():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

def get_kst_today_str():
    return datetime.now(ZoneInfo('Asia/Seoul')).strftime("%Y-%m-%d")

def markdown_to_html(text):
    return markdown.markdown(text) if text else ""

class NewsScraper:
    def __init__(self, config):
        self.config = config
        self.session = self._create_session()

    def _create_session(self):
        session = requests.Session()
        session.mount('https://', CustomHttpAdapter())
        return session

    # â¬‡ï¸ (ìˆ˜ì •) ì´ë¯¸ì§€ ìŠ¤í¬ë ˆì´í•‘ ë¡œì§ ì „ì²´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.
    def get_image_url(self, article_url: str) -> str:
        logging.info(f" -> ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {article_url[:80]}...")
        try:
            headers = { "User-Agent": random.choice(self.config.USER_AGENTS) }
            response = self.session.get(article_url, headers=headers, timeout=10, allow_redirects=True)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            
            # 1ìˆœìœ„: Open Graph ë° íŠ¸ìœ„í„° ì¹´ë“œ ë©”íƒ€ íƒœê·¸ (ê°€ì¥ ì •í™•ë„ê°€ ë†’ìŒ)
            meta_image = soup.find("meta", property="og:image") or soup.find("meta", name="twitter:image")
            if meta_image and meta_image.get("content"):
                meta_url = self._resolve_url(article_url, meta_image["content"])
                if self._is_valid_candidate(meta_url) and self._validate_image(meta_url):
                    logging.info(" -> âœ… 1ìˆœìœ„(ë©”íƒ€ íƒœê·¸)ì—ì„œ ê³ í™”ì§ˆ ì´ë¯¸ì§€ ë°œê²¬!")
                    return meta_url

            # 2ìˆœìœ„: ë³¸ë¬¸ ë‚´ì˜ figure ë˜ëŠ” picture íƒœê·¸ (ì£¼ë¡œ ëŒ€í‘œ ì´ë¯¸ì§€)
            for tag in soup.select('figure > img, picture > img, .article_photo img, .photo_center img', limit=5):
                img_url = tag.get('src') or tag.get('data-src') or (tag.get('srcset').split(',')[0].strip().split(' ')[0] if tag.get('srcset') else None)
                if img_url and self._is_valid_candidate(img_url):
                    full_url = self._resolve_url(article_url, img_url)
                    if self._validate_image(full_url):
                        logging.info(" -> âœ… 2ìˆœìœ„(ë³¸ë¬¸ ëŒ€í‘œ íƒœê·¸)ì—ì„œ ê³ í™”ì§ˆ ì´ë¯¸ì§€ ë°œê²¬!")
                        return full_url
            
            # 3ìˆœìœ„: ë³¸ë¬¸ì˜ ëª¨ë“  img íƒœê·¸ (ê°€ì¥ ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            for img in soup.find_all("img", limit=10):
                img_url = img.get("src") or img.get("data-src")
                if img_url and self._is_valid_candidate(img_url):
                    full_url = self._resolve_url(article_url, img_url)
                    if self._validate_image(full_url):
                        logging.info(" -> âœ… 3ìˆœìœ„(ë³¸ë¬¸ ì „ì²´)ì—ì„œ ì´ë¯¸ì§€ ë°œê²¬.")
                        return full_url

            logging.warning(f" -> âš ï¸ ìœ íš¨ ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í•¨: {article_url[:80]}...")
            return self.config.DEFAULT_IMAGE_URL
        except Exception:
            logging.error(f" -> ğŸš¨ ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {article_url[:80]}...", exc_info=True)
            return self.config.DEFAULT_IMAGE_URL

    def _resolve_url(self, base_url, image_url):
        if image_url.startswith('//'): return 'https:' + image_url
        return urljoin(base_url, image_url)

    def _is_valid_candidate(self, image_url):
        if 'news.google.com' in image_url or 'lh3.googleusercontent.com' in image_url: return False
        # (ìˆ˜ì •) ë¡œê³ ë‚˜ ì•„ì´ì½˜ ê°™ì€ ì´ë¯¸ì§€ íŒ¨í„´ì„ ë” ì ê·¹ì ìœ¼ë¡œ í•„í„°ë§
        unwanted_patterns = self.config.UNWANTED_IMAGE_PATTERNS + ['logo', 'icon', 'ci', 'bi', 'symbol', 'banner']
        return not any(pattern in image_url.lower() for pattern in unwanted_patterns)

    def _validate_image(self, image_url):
        """ì´ë¯¸ì§€ë¥¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í¬ê¸°ì™€ ë¹„ìœ¨ì„ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜"""
        try:
            response = self.session.get(image_url, stream=True, timeout=5)
            response.raise_for_status()
            
            content_type = response.headers.get('Content-Type', '').lower()
            if 'image' not in content_type: return False
            
            # (ì¶”ê°€) ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ ì´ë¯¸ì§€ ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ê±´ë„ˆë›°ê¸° (íš¨ìœ¨ì„±)
            if 'content-length' in response.headers and int(response.headers['content-length']) < 10000: # 10KB ì´í•˜
                return False

            img_data = BytesIO(response.content)
            with Image.open(img_data) as img:
                width, height = img.size
                # (ìˆ˜ì •) ìµœì†Œ ê°€ë¡œ/ì„¸ë¡œ í¬ê¸° ê¸°ì¤€ì„ ë†’ì—¬ ì‘ì€ ì¸ë„¤ì¼ ì œì™¸
                if width < self.config.MIN_IMAGE_WIDTH or height < self.config.MIN_IMAGE_HEIGHT:
                    return False
                # (ìˆ˜ì •) ê°€ë¡œê°€ ë” ê¸´ ì´ë¯¸ì§€ë¥¼ ì„ í˜¸í•˜ë„ë¡ ë¹„ìœ¨ ì¡°ì • (ë‰´ìŠ¤ ì´ë¯¸ì§€ëŠ” ë³´í†µ ê°€ë¡œê°€ ê¹€)
                aspect_ratio = width / height
                if aspect_ratio > 4.0 or aspect_ratio < 0.5: # ë„ˆë¬´ ê¸¸ê±°ë‚˜ ì„¸ë¡œë¡œ ê¸´ ì´ë¯¸ì§€ ì œì™¸
                    return False
                if aspect_ratio < 1.2: # ê°€ë¡œê°€ ì„¸ë¡œë³´ë‹¤ 1.2ë°° ì´ìƒ ê¸¸ì–´ì•¼ í•¨
                    return False
                return True
        except Exception:
            return False
    # â¬†ï¸ ì´ë¯¸ì§€ ìŠ¤í¬ë ˆì´í•‘ ë¡œì§ ê°œì„  ì™„ë£Œ

class AIService:
    def __init__(self, config):
        self.config = config
        # OpenAI API í‚¤ ìœ íš¨ì„± ê²€ì‚¬
        if not self.config.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)
    def _call_openai_api(self, system_prompt, user_prompt, is_json=False):
        """OpenAI APIë¥¼ í˜¸ì¶œí•˜ëŠ” ì¤‘ì•™ í•¨ìˆ˜"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        try:
            response_format = {"type": "json_object"} if is_json else {"type": "text"}
            
            response = self.client.chat.completions.create(
                model=self.config.OPENAI_MODEL,
                messages=messages,
                temperature=0.7,
                response_format=response_format
            )
            content = response.choices[0].message.content.strip()
            
            if is_json:
                # JSON í˜•ì‹ì¸ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸
                json.loads(content)
            
            return content
        except Exception as e:
            logging.error(f" -> ğŸš¨ OpenAI API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
            return None    

    def generate_single_summary(self, article_title: str, article_text: str) -> str:
        logging.info(f" -> ChatGPT ìš”ì•½ ìƒì„± ìš”ì²­: {article_title}")
        if not article_text or len(article_text) < 100:
            logging.warning(" -> âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return "ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        system_prompt = "ë‹¹ì‹ ì€ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ëŠ” ë‰´ìŠ¤ ì—ë””í„°ì…ë‹ˆë‹¤. ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ë…ìë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ 3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."
        user_prompt = f"[ì œëª©]: {article_title}\n[ë³¸ë¬¸]:\n{article_text[:2000]}"
        
        summary = self._call_openai_api(system_prompt, user_prompt)
        
        if summary:
            logging.info(" -> âœ… ChatGPT ìš”ì•½ ìƒì„± ì„±ê³µ.")
            return summary
        else:
            return "AI ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _generate_content_with_retry(self, prompt, is_json=False):
        for attempt in range(3):
            try:
                response = self.model.generate_content(prompt)
                if is_json:
                    cleaned_text = response.text.strip().replace("```json", "").replace("```", "")
                    json.loads(cleaned_text)
                    return cleaned_text
                return response.text
            except Exception as e:
                logging.warning(f"AI ìƒì„± ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3): {e}")
                time.sleep(2 ** attempt)
        return None

    def select_top_news(self, news_list):
        logging.info(f"ChatGPT ë‰´ìŠ¤ ì„ ë³„ ì‹œì‘... (ëŒ€ìƒ: {len(news_list)}ê°œ)")
        context = "\n\n".join([f"ê¸°ì‚¬ #{i}\nì œëª©: {news['title']}\nìš”ì•½: {news['summary']}" for i, news in enumerate(news_list)])
        
        system_prompt = """
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ 'ë¬¼ë¥˜ ì „ë¬¸' ë‰´ìŠ¤ ì—ë””í„°ì…ë‹ˆë‹¤. 
        ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” í™”ë¬¼ì°¨ ìš´ì†¡, ì£¼ì„ , ìœ¡ìƒ ìš´ì†¡, ê³µê¸‰ë§ ê´€ë¦¬(SCM) ë¶„ì•¼ì˜ ì¢…ì‚¬ìë“¤ì—ê²Œ ê°€ì¥ ì‹¤ìš©ì ì´ê³  ì¤‘ìš”í•œ ìµœì‹  ì •ë³´ë¥¼ ì„ ë³„í•˜ì—¬ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
        ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì˜ ì—„ê²©í•œ ê¸°ì¤€ì— ë”°ë¼ ìµœì¢… Top 10 ë‰´ìŠ¤ë¥¼ ì„ ì •í•´ì£¼ì„¸ìš”.

        [ì„ ë³„ ê¸°ì¤€]
        1.  **í•µì‹¬ ì£¼ì œ ì§‘ì¤‘:** ë°˜ë“œì‹œ ì•„ë˜ ë¶„ì•¼ì™€ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‰´ìŠ¤ë§Œ ì„ ì •í•´ì•¼ í•©ë‹ˆë‹¤.
            - í™”ë¬¼ ìš´ì†¡ ë° íŠ¸ëŸ­í‚¹ ë™í–¥ (í™”ë¬¼ì°¨, ìš´ì„, ìœ ê°€ ë“±)
            - ì£¼ì„ ì‚¬ ë° ìš´ì†¡ì‚¬ ì†Œì‹ (M&A, ì‹ ê·œ ì„œë¹„ìŠ¤, ì‹¤ì  ë°œí‘œ ë“±)
            - ë¬¼ë¥˜ ê¸°ìˆ (Logi-Tech), í”Œë«í¼, ìŠ¤íƒ€íŠ¸ì—… ì†Œì‹
            - í’€í•„ë¨¼íŠ¸, ì°½ê³  ìë™í™”, ë¼ìŠ¤íŠ¸ë§ˆì¼ ë°°ì†¡
            - ê³µê¸‰ë§ ê´€ë¦¬(SCM) ìµœì‹  ì „ëµ
            - ì •ë¶€ì˜ ë¬¼ë¥˜/ìš´ì†¡ ê´€ë ¨ ì •ì±… ë° ê·œì œ ë³€ê²½
        2.  **ê´€ë ¨ì„± ë‚®ì€ ì£¼ì œ ì œì™¸:** IT, ë°˜ë„ì²´, ìë™ì°¨ ë“± ë‹¤ë¥¸ ì‚°ì—… ë‰´ìŠ¤ëŠ” ë¬¼ë¥˜ì™€ ì§ì ‘ì ì¸ ì—°ê´€ì„±ì´ ì–¸ê¸‰ëœ ê²½ìš°ì—ë§Œ í¬í•¨í•©ë‹ˆë‹¤.
        3.  **í•´ìš´/í•­ë§Œ ë‰´ìŠ¤ ë¹„ì¤‘ ìœ ì§€:** í•´ì–‘, í•­ë§Œ, ì„ ë°• ê´€ë ¨ ë‰´ìŠ¤ëŠ” ì—¬ì „íˆ ì „ì²´ 10ê°œ ì¤‘ **ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ** í¬í•¨í•˜ì—¬ ìœ¡ìƒ ìš´ì†¡ ìœ„ì£¼ì˜ ê· í˜•ì„ ë§ì¶°ì£¼ì„¸ìš”.
        4.  **ì¤‘ë³µ ì œê±°:** ë‚´ìš©ì´ ì‚¬ì‹¤ìƒ ë™ì¼í•œ ë‰´ìŠ¤ëŠ” ë‹¨ í•˜ë‚˜ë§Œ ì„ ì •í•©ë‹ˆë‹¤. ì œëª©ì´ ê°€ì¥ êµ¬ì²´ì ì´ê³  ì •ë³´ê°€ í’ë¶€í•œ ê¸°ì‚¬ë¥¼ ëŒ€í‘œë¡œ ì„ íƒí•˜ì„¸ìš”.
        5.  **ì¤‘ìš”ë„ ìˆœì„œ:** ìœ„ ê¸°ì¤€ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” í›„ë³´ë“¤ ì¤‘ì—ì„œ, ì—…ê³„ ì¢…ì‚¬ìì—ê²Œ ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì¤‘ìš”ë„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ì£¼ì„¸ìš”.

        [ì¶œë ¥ í˜•ì‹]
        - ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
        - 'selected_indices' í‚¤ì— ë‹¹ì‹ ì´ ìµœì¢… ì„ ì •í•œ ê¸°ì‚¬ 10ê°œì˜ ë²ˆí˜¸(ì¸ë±ìŠ¤)ë¥¼ **ì¤‘ìš”ë„ ìˆœì„œëŒ€ë¡œ** ìˆ«ì ë°°ì—´ë¡œ ë‹´ì•„ì£¼ì„¸ìš”.
        ì˜ˆ: {"selected_indices": [3, 15, 4, 8, 22, 1, 30, 11, 19, 5]}
        """
        user_prompt = f"[ë‰´ìŠ¤ ëª©ë¡]\n{context}"
        
        response_text = self._call_openai_api(system_prompt, user_prompt, is_json=True)

        if response_text:
            try:
                selected_indices = json.loads(response_text).get('selected_indices', [])
                top_news = [news_list[i] for i in selected_indices if i < len(news_list)]
                logging.info(f"âœ… ChatGPTê°€ {len(top_news)}ê°œ ë‰´ìŠ¤ë¥¼ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.")
                return top_news
            except (json.JSONDecodeError, KeyError) as e:
                logging.error(f"âŒ ChatGPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}. ìƒìœ„ 10ê°œ ë‰´ìŠ¤ë¥¼ ì„ì˜ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")
        return news_list[:10]

    def generate_briefing(self, news_list):
        logging.info("ChatGPT ë¸Œë¦¬í•‘ ìƒì„± ì‹œì‘...")
        context = "\n\n".join([f"ì œëª©: {news['title']}\nìš”ì•½: {news.get('ai_summary') or news.get('summary')}" for news in news_list])
        
        system_prompt = """
        ë‹¹ì‹ ì€ íƒì›”í•œ í†µì°°ë ¥ì„ ê°€ì§„ ë¬¼ë¥˜/ê²½ì œ ë‰´ìŠ¤ íë ˆì´í„°ì…ë‹ˆë‹¤. ì•„ë˜ ë‰´ìŠ¤ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, ë…ìë¥¼ ìœ„í•œ ë§¤ìš° ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ 'ë°ì¼ë¦¬ ë¸Œë¦¬í•‘'ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        **ì¶œë ¥ í˜•ì‹ ê·œì¹™:**
        1. 'ì—ë””í„° ë¸Œë¦¬í•‘'ì€ '## ì—ë””í„° ë¸Œë¦¬í•‘' í—¤ë”ë¡œ ì‹œì‘í•˜ë©°, ì˜¤ëŠ˜ ë‰´ìŠ¤ì˜ í•µì‹¬ì„ 2~3 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
        2. 'ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„'ì€ '## ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„' í—¤ë”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.
        3. ì£¼ìš” ë‰´ìŠ¤ ë¶„ì„ì—ì„œëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ 2~3ê°œë¥¼ '###' í—¤ë”ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.
        4. ê° ì¹´í…Œê³ ë¦¬ ì•ˆì—ì„œëŠ”, ê´€ë ¨ëœ ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³  ê¸€ë¨¸ë¦¬ ê¸°í˜¸(`*`)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        5. ë¬¸ì¥ ì•ˆì—ì„œ ê°•ì¡°í•˜ê³  ì‹¶ì€ íŠ¹ì • í‚¤ì›Œë“œëŠ” í°ë”°ì˜´í‘œ(" ")ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”.
        """
        user_prompt = f"[ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ëª©ë¡]\n{context}"

        briefing = self._call_openai_api(system_prompt, user_prompt)
        
        if briefing:
            logging.info("âœ… ChatGPT ë¸Œë¦¬í•‘ ìƒì„± ì„±ê³µ!")
            return briefing
        else:
            logging.warning("âš ï¸ ChatGPT ë¸Œë¦¬í•‘ ìƒì„± ì‹¤íŒ¨.")
            return "ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

class NewsService:
    def __init__(self, config, scraper, ai_service):
        self.config = config
        self.scraper = scraper
        self.ai_service = ai_service
        self.sent_links = self._load_sent_links()

    def __del__(self):
        pass

    def _load_sent_links(self):
        try:
            with open(self.config.SENT_LINKS_FILE, 'r', encoding='utf-8') as f:
                links = set(line.strip() for line in f)
                logging.info(f"âœ… {len(links)}ê°œ ë°œì†¡ ê¸°ë¡ ë¡œë“œ ì™„ë£Œ.")
                return links
        except FileNotFoundError:
            logging.warning("âš ï¸ ë°œì†¡ ê¸°ë¡ íŒŒì¼ì´ ì—†ì–´ ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return set()

    def _fetch_rss_feeds(self):
        logging.info("ğŸ†• ì—¬ëŸ¬ RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤... (ì´ {}ê°œ ì†ŒìŠ¤)".format(len(self.config.RSS_FEEDS)))
        all_entries = []
        headers = {"User-Agent": random.choice(self.config.USER_AGENTS)}
        for rss_url in self.config.RSS_FEEDS:
            try:
                response = requests.get(rss_url, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.content, 'xml')
                entries = [{
                    'rss_title': item.title.text if item.title else "",
                    'link': item.link.text if item.link else "",
                    'rss_summary': item.description.text if item.description else ""
                } for item in soup.find_all('item')]
                all_entries.extend(entries)
                logging.info(f"âœ… {rss_url}ì—ì„œ {len(entries)}ê°œ entry ìˆ˜ì§‘ ì™„ë£Œ.")
            except Exception as e:
                logging.warning(f"âš ï¸ {rss_url} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        logging.info(f"ì´ {len(all_entries)}ê°œì˜ í›„ë³´ ê¸°ì‚¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return all_entries

    def get_fresh_news(self):
        try:
            initial_articles = self._fetch_rss_feeds()
            logging.info(f"ì´ {len(initial_articles)}ê°œì˜ ìƒˆë¡œìš´ í›„ë³´ ê¸°ì‚¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            processed_articles = []
            with ThreadPoolExecutor(max_workers=5) as executor:
                future_to_entry = {executor.submit(self._resolve_and_process_article, entry): entry for entry in initial_articles[:self.config.MAX_ARTICLES] if entry['link'] not in self.sent_links}
                for future in as_completed(future_to_entry):
                    article_data = future.result()
                    if article_data:
                        processed_articles.append(article_data)
            logging.info(f"âœ… ì´ {len(processed_articles)}ê°œì˜ ìœ íš¨í•œ ìƒˆ ë‰´ìŠ¤ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            return processed_articles
        except Exception:
            logging.error("âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ:", exc_info=True)
            return []
            
    def _clean_url(self, url: str) -> str | None:
        try:
            parsed = urlparse(url)
            if any(ad_domain in parsed.netloc for ad_domain in self.config.AD_DOMAINS_BLACKLIST):
                return None
            return parsed._replace(fragment="").geturl()
        except Exception:
            return None

    def _resolve_and_process_article(self, entry):
        logging.info(f"-> URL ì²˜ë¦¬ ì‹œë„: {entry['rss_title']}")
        try:
            cleaned_url = self._clean_url(entry['link'])
            if not cleaned_url:
                logging.warning(f" -> âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ URL: {entry['rss_title']}")
                return None
            
            article = Article(cleaned_url, language='ko') 
            article.download()
            article.parse()
            
            if article.meta_lang != 'ko':
                logging.info(f" -> ğŸŒ í•œêµ­ì–´ ê¸°ì‚¬ê°€ ì•„ë‹ˆë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤: (ì–¸ì–´: {article.meta_lang}) {article.title}")
                return None

            if not article.text and not article.title:
                logging.warning(f" -> âš ï¸ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ (403 Forbidden ë“±): {cleaned_url}")
                return None

            final_title = article.title if article.title else entry['rss_title']
            logging.info(f" -> âœ… [í•œêµ­ì–´ ë‰´ìŠ¤] ìµœì¢… URL/ì œëª© í™•ë³´: {final_title}")
            
            final_url = article.url 

            return {
                'title': final_title,
                'link': final_url,
                'url': final_url,
                'summary': BeautifulSoup(entry.get('rss_summary', ''), 'lxml').get_text(strip=True)[:150] + "...",
                'image_url': self.scraper.get_image_url(final_url),
                'full_text': article.text
            }
        except ArticleException as e:
            logging.error(f" -> ğŸš¨ ê¸°ì‚¬ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë¥˜: {e}")
            return None
        except Exception:
            logging.error(f" -> ğŸš¨ URL ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {entry['rss_title']}", exc_info=True)
            return None

    def update_sent_links_log(self, news_list):
        links = [news['link'] for news in news_list]
        try:
            with open(self.config.SENT_LINKS_FILE, 'a', encoding='utf-8') as f:
                for link in links: f.write(link + '\n')
            logging.info(f"âœ… {len(links)}ê°œ ë§í¬ë¥¼ ë°œì†¡ ê¸°ë¡ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logging.error("âŒ ë°œì†¡ ê¸°ë¡ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:", exc_info=True)

class EmailService:
    def __init__(self, config):
        self.config = config
        self.credentials = self._get_credentials()

    def _get_credentials(self):
        creds = None
        if os.path.exists(self.config.TOKEN_FILE):
            creds = Credentials.from_authorized_user_file(self.config.TOKEN_FILE, ['https://www.googleapis.com/auth/gmail.send'])
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                flow = InstalledAppFlow.from_client_secrets_file(self.config.CREDENTIALS_FILE, ['https://www.googleapis.com/auth/gmail.send'])
                creds = flow.run_local_server(port=0)
            with open(self.config.TOKEN_FILE, 'w') as token:
                token.write(creds.to_json())
        return creds

    def create_email_body(self, news_list, ai_briefing_html, today_date_str):
        env = Environment(loader=FileSystemLoader('.'))
        template = env.get_template('email_template.html')
        return template.render(news_list=news_list, today_date=today_date_str, ai_briefing=ai_briefing_html)
    
    def send_email(self, subject, body_html, news_list):
        if not self.config.RECIPIENT_LIST:
            logging.warning("âŒ ìˆ˜ì‹ ì ëª©ë¡ì´ ë¹„ì–´ìˆì–´ ì´ë©”ì¼ì„ ë°œì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        try:
            service = build('gmail', 'v1', credentials=self.credentials)
            msg = MIMEMultipart('related')
            msg['To'] = ", ".join(self.config.RECIPIENT_LIST)
            msg['From'] = self.config.SENDER_EMAIL
            msg['Subject'] = subject
            msg_alternative = MIMEMultipart('alternative')
            msg_alternative.attach(MIMEText(body_html, 'html', 'utf-8'))
            msg.attach(msg_alternative)

            for news in news_list:
                if news.get('image_data'):
                    image = MIMEImage(news['image_data'])
                    image.add_header('Content-ID', f"<{news['cid']}>")
                    msg.attach(image)

            encoded_message = base64.urlsafe_b64encode(msg.as_bytes()).decode()
            create_message = {'raw': encoded_message}
            
            send_message = service.users().messages().send(userId="me", body=create_message).execute()
            logging.info(f"âœ… ì´ë©”ì¼ ë°œì†¡ ì„±ê³µ! (Message ID: {send_message['id']})")
            
            # â¬‡ï¸ (ì¶”ê°€) ë°œì†¡ëœ ë‰´ìŠ¤ ëª©ë¡ì„ ë¡œê·¸ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.
            logging.info("--- ğŸ“§ ë°œì†¡ëœ ë‰´ìŠ¤ë ˆí„° ëª©ë¡ ---")
            for i, news in enumerate(news_list):
                logging.info(f"  {i+1}. {news['title']}")
                logging.info(f"     - ë§í¬: {news['link']}")
            logging.info("-----------------------------")
            # â¬†ï¸ ë¡œê·¸ ê¸°ë¡ ì¶”ê°€ ì™„ë£Œ

        except HttpError:
            logging.error("âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨:", exc_info=True)

def main():
    setup_logging()
    logging.info("ğŸš€ ë‰´ìŠ¤ë ˆí„° ìë™ ìƒì„± í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    news_service = None
    try:
        config = Config()
        news_scraper = NewsScraper(config)
        ai_service = AIService(config)
        news_service = NewsService(config, news_scraper, ai_service)
        
        all_news = news_service.get_fresh_news()
        if not all_news:
            logging.info("â„¹ï¸ ë°œì†¡í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
            
        top_10_news_base = ai_service.select_top_news(all_news)
        if not top_10_news_base:
            logging.warning("âš ï¸ AIê°€ Top ë‰´ìŠ¤ë¥¼ ì„ ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return
            
        logging.info(f"âœ… AI Top 10 ì„ ë³„ ì™„ë£Œ. ì„ ë³„ëœ {len(top_10_news_base)}ê°œ ë‰´ìŠ¤ì˜ ê°œë³„ AI ìš”ì•½ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_to_news = {executor.submit(ai_service.generate_single_summary, news['title'], news['full_text']): news for news in top_10_news_base}
            for future in as_completed(future_to_news):
                news = future_to_news[future]
                try:
                    summary = future.result()
                    # â¬‡ï¸ (ìˆ˜ì •) AI ìš”ì•½ ê²°ê³¼ê°€ ë¹„ì •ìƒì ì¼ ê²½ìš° ëŒ€ì²´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                    if "ì˜¤ë¥˜" in summary or "ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in summary or len(summary) < 20:
                        logging.warning(f" -> âš ï¸ AI ìš”ì•½ ì‹¤íŒ¨, ëŒ€ì²´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤: {news['title']}")
                        # ê¸°ì‚¬ ë³¸ë¬¸ì˜ ì²« 200ìë¥¼ ê°€ì ¸ì™€ì„œ ë¬¸ì¥ì„ ë§ˆë¬´ë¦¬í•˜ê³  "..."ë¥¼ ë¶™ì…ë‹ˆë‹¤.
                        clean_text = re.sub(r'\s+', ' ', news['full_text']).strip()
                        end_index = clean_text.find('.', 150) # 150ì ê·¼ì²˜ì˜ ì²« ë§ˆì¹¨í‘œë¥¼ ì°¾ìŒ
                        if end_index != -1:
                            news['ai_summary'] = clean_text[:end_index+1]
                        else:
                            news['ai_summary'] = clean_text[:200] + "..."
                    else:
                        news['ai_summary'] = summary
                except Exception as e:
                    logging.error(f" -> ğŸš¨ AI ìš”ì•½ ìŠ¤ë ˆë“œì—ì„œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    news['ai_summary'] = news['summary'] # RSS ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´
        # â¬†ï¸ ìˆ˜ì • ì™„ë£Œ
        
        ai_briefing_md = ai_service.generate_briefing(top_10_news_base)
        ai_briefing_html = markdown_to_html(ai_briefing_md)

        logging.info("ğŸ“§ ì´ë©”ì¼ ë°œì†¡ì„ ìœ„í•´ ë‰´ìŠ¤ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤...")
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_news = {}
            for i, news in enumerate(top_10_news_base):
                news['cid'] = f"image_{i}_{int(time.time())}"
                if news.get('image_url') and news['image_url'] != config.DEFAULT_IMAGE_URL:
                    future_to_news[executor.submit(news_scraper.session.get, news['image_url'], timeout=10)] = news
                else:
                    news['image_data'] = None

            for future in as_completed(future_to_news):
                news = future_to_news[future]
                try:
                    response = future.result()
                    response.raise_for_status()
                    news['image_data'] = response.content
                    logging.info(f" -> âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {news['title'][:30]}...")
                except Exception as e:
                    news['image_data'] = None
                    logging.warning(f" -> âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {news['title'][:30]}... ({e})")

        email_service = EmailService(config)
        today_str = get_kst_today_str()
        email_subject = f"[{today_str}] ì˜¤ëŠ˜ì˜ IT/ì‚°ì—… ë‰´ìŠ¤ Top {len(top_10_news_base)}"
        
        email_body = email_service.create_email_body(top_10_news_base, ai_briefing_html, today_str)
        email_service.send_email(email_subject, email_body, top_10_news_base)
        
        news_service.update_sent_links_log(top_10_news_base)
        logging.info("ğŸ‰ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except (ValueError, FileNotFoundError) as e:
        logging.critical(f"ğŸš¨ ì„¤ì • ë˜ëŠ” íŒŒì¼ ì˜¤ë¥˜: {e}")
    except Exception:
        logging.critical("ğŸ”¥ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ:", exc_info=True)
    finally:
        if news_service:
            del news_service

if __name__ == "__main__":
    main()


